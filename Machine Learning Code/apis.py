# -*- coding: utf-8 -*-
"""APIS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_qn8bkNADhcYB4zYo15MhdiVID9I9SdL
"""

from google.colab import drive
drive.mount('/content/drive')

"""# **<h1>Importing Libraries**"""

!pip install transformers
!pip install flask-ngrok
!pip install "pymongo[srv]"
!pip install langdetect
import re
import numpy as np
import pandas as pd
import torch
import nltk
from nltk.corpus import stopwords
nltk.download("stopwords")
stop_words = set(stopwords.words("english"))
from textblob import TextBlob
from sklearn.preprocessing import MinMaxScaler
from transformers import BertTokenizer
import torch
import torch.nn as nn
from transformers import BertModel
import torch.nn.functional as F
from sklearn.preprocessing import LabelEncoder
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from transformers import AdamW, get_linear_schedule_with_warmup
# Load the BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)
from flask import Flask,request
from flask_ngrok import run_with_ngrok
from itertools import chain
from flask import jsonify
import json
import requests
import pymongo
from pymongo import MongoClient
import random
import math
import string
from langdetect import detect

"""# **<h1>Get GPU Support**"""

if torch.cuda.is_available():       
    device = torch.device("cuda")
    print(f'There are {torch.cuda.device_count()} GPU(s) available.')
    print('Device name:', torch.cuda.get_device_name(0))

else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")

"""# **<h1>Data Preprocessing**"""

def text_preprocessing(text):
    """
    - Remove entity mentions (eg. '@united')
    - Correct errors (eg. '&amp;' to '&')
    @param    text (str): a string to be processed.
    @return   text (Str): the processed string.
    """
    '''# Remove '@name'
    text = re.sub(r'(@.*?)[\s]', ' ', text)

    # Replace '&amp;' with '&'
    text = re.sub(r'&amp;', '&', text)

    # Remove trailing whitespace
    text = re.sub(r'\s+', ' ', text).strip()'''

    # lowercasing the text
    text = text.lower()
    # encoding the text to ASCII format
    text_encode = text.encode(encoding="ascii", errors="ignore")
    # decoding the text
    text_decode = text_encode.decode()
    # removing mentions 
    text = re.sub("@\S+", "", text)
    # remove market tickers
    text = re.sub("\$", "", text)
    # remove urls
    text = re.sub("https?:\/\/.*[\r\n]*", "", text)
    # removing hashtags
    text = re.sub("#", "", text)
    # remove punctuation
    punct = set(string.punctuation) 
    text = "".join([ch for ch in text if ch not in punct])
    # cleaning the text to remove extra whitespace 
    text = " ".join([word for word in text.split()])


    return text

"""## **<h1>Initializing BERT Model**"""

def initialize_model(epochs=4):
    """Initialize the Bert Classifier, the optimizer and the learning rate scheduler.
    """
    # Instantiate Bert Classifier
    bert_classifier = BertClassifier(freeze_bert=False)

    # Tell PyTorch to run the model on GPU
    bert_classifier.to(device)

    # Create the optimizer
    optimizer = AdamW(bert_classifier.parameters(),
                      lr=5e-5,    # Default learning rate
                      eps=1e-8    # Default epsilon value
                      )

    # Total number of training steps
    total_steps = len(train_dataloader) * epochs

    # Set up the learning rate scheduler
    scheduler = get_linear_schedule_with_warmup(optimizer,
                                                num_warmup_steps=0, # Default value
                                                num_training_steps=total_steps)
    return bert_classifier, optimizer, scheduler

"""## **<h1>Specifying max lengths for sentences**"""

# Specify `MAX_LEN`
MAX_LEN = 512

"""## **<h1>Setting BERT Parameters**"""

# Create a function to tokenize a set of texts
def preprocessing_for_bert(data):
    """Perform required preprocessing steps for pretrained BERT.
    @param    data (np.array): Array of texts to be processed.
    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.
    @return   attention_masks (torch.Tensor): Tensor of indices specifying which
                  tokens should be attended to by the model.
    """
    # Create empty lists to store outputs
    input_ids = []
    attention_masks = []

    # For every sentence...
    for sent in data:
        # `encode_plus` will:
        #    (1) Tokenize the sentence
        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end
        #    (3) Truncate/Pad sentence to max length
        #    (4) Map tokens to their IDs
        #    (5) Create attention mask
        #    (6) Return a dictionary of outputs
        encoded_sent = tokenizer.encode_plus(
            text=text_preprocessing(sent),  # Preprocess sentence
            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`
            max_length=MAX_LEN,                  # Max length to truncate/pad
            pad_to_max_length=True,         # Pad sentence to max length
            #return_tensors='pt',           # Return PyTorch tensor
            return_attention_mask=True      # Return attention mask
            )
        
        # Add the outputs to the lists
        input_ids.append(encoded_sent.get('input_ids'))
        attention_masks.append(encoded_sent.get('attention_mask'))

    # Convert lists to tensors
    input_ids = torch.tensor(input_ids)
    attention_masks = torch.tensor(attention_masks)

    return input_ids, attention_masks

"""## **<h1>Create BERT Class**"""

# Create the BertClassfier class
class BertClassifier(nn.Module):
    """Bert Model for Classification Tasks.
    """
    def __init__(self, freeze_bert=False):
        """
        @param    bert: a BertModel object
        @param    classifier: a torch.nn.Module classifier
        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model
        """
        super(BertClassifier, self).__init__()
        # Specify hidden size of BERT, hidden size of our classifier, and number of labels
        D_in, H, D_out = 768, 50, 2

        # Instantiate BERT model
        self.bert = BertModel.from_pretrained('bert-base-uncased')

        # Instantiate an one-layer feed-forward classifier
        self.classifier = nn.Sequential(
            nn.Linear(D_in, H),
            nn.ReLU(),
            #nn.Dropout(0.5),
            nn.Linear(H, D_out)
        )

        # Freeze the BERT model
        if freeze_bert:
            for param in self.bert.parameters():
                param.requires_grad = False

    def forward(self, input_ids, attention_mask):
        """
        Feed input to BERT and the classifier to compute logits.
        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,
                      max_length)
        @param    attention_mask (torch.Tensor): a tensor that hold attention mask
                      information with shape (batch_size, max_length)
        @return   logits (torch.Tensor): an output tensor with shape (batch_size,
                      num_labels)
        """
        # Feed input to BERT
        outputs = self.bert(input_ids=input_ids,
                            attention_mask=attention_mask)
        
        # Extract the last hidden state of the token `[CLS]` for classification task
        last_hidden_state_cls = outputs[0][:, 0, :]

        # Feed input to classifier to compute logits
        logits = self.classifier(last_hidden_state_cls)

        return logits

"""## **<h1>Putting Model in Evaluation Phase**"""

def bert_predict(model, test_dataloader):
    """Perform a forward pass on the trained BERT model to predict probabilities
    on the test set.
    """
    # Put the model into the evaluation mode. The dropout layers are disabled during
    # the test time.
    model.eval()

    all_logits = []

    # For each batch in our test set...
    for batch in test_dataloader:
        # Load batch to GPU
        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]

        # Compute logits
        with torch.no_grad():
            logits = model(b_input_ids, b_attn_mask)
        all_logits.append(logits)
    
    # Concatenate logits from each batch
    all_logits = torch.cat(all_logits, dim=0)

    # Apply softmax to calculate probabilities
    probs = F.softmax(all_logits, dim=1).cpu().numpy()

    return probs

"""# **<h1>Loading the BERT**"""

# Loading Trained BERT Model
 model=torch.load("/content/drive/MyDrive/FYP Datasets/trained_model1.pth") 
# model=torch.load("/content/drive/MyDrive/FYP Datasets/trained_model1.pth",map_location=torch.device('cpu'))

"""# **<h1>Initializing TextBlob for Neutral Reviews**"""

def use_TextBlob (line):
  anaylsis=TextBlob(line).sentiment
  if anaylsis[0] > 0 : 
    return 2 
  elif anaylsis[0] < 0 : 
    return 0 
  else : 
    return 1

"""# **<h1>Converting API Data into Dataframe**"""

#Function that creates dataframe of json file
def jsonToDataframe(df1): 

  df2=[]
 
  #Simple dataframe which will be like above one
  for business in df1['data']:
    obj={'name':'','google_ratings':0,'reviews':'','rating':0}
    obj['name']=business['name']
    obj['google_ratings']=business['rating']
    for review in business['reviews']:
      df2.append({'name': obj['name'],'google_ratings':obj['google_ratings'],'reviews':review['text'],'rating':review['rating']})

  dataframe = pd.DataFrame.from_dict(df2)
  #Empty dataframe and initialing its columns
  df=pd.DataFrame()
  df["name"]=""
  df["reviews"]=""
  df["google_rating"]=0
  df["sentiment_score"]=0
  df["system_rating"]=0
  df["averaged_rating"]=0
  #Get each object of json 1 by 1
  for index,val in dataframe.iterrows():
    df=df.append({"name":val["name"],"reviews":val['reviews'],"google_rating":val['google_ratings'],"averaged_rating":val['rating']},ignore_index=True)
  return df

"""# **<h1>Getting Predictions on API Data**"""

def getPredictions(df):
  # Run `preprocessing_for_bert` on the prediction set
  pre_inputs, pre_masks = preprocessing_for_bert(df['reviews'])

  # Create the DataLoader for our prediction set
  pre_dataset = TensorDataset(pre_inputs, pre_masks)
  pre_sampler = SequentialSampler(pre_dataset)
  pre_dataloader = DataLoader(pre_dataset, sampler=pre_sampler, batch_size=10)

  #Compute predicted probabilities on the test set
  probs = bert_predict(model, pre_dataloader)

  # Get predictions from the probabilities
  threshold = 0.5

  return np.where(probs[:, 1] > threshold, 2, 0)

"""# **<h1>Getting Neutral Reviews**"""

def getNeutral(df):
  for index,row in enumerate(df['reviews']):
    sentiment=use_TextBlob(row)
    if sentiment==1:
      df['sentiment_score'][index]=1
  return df

"""# **<h1>Calculating System Rating**"""

#this function takes dataframe with prediction column and return the system genrated score column data frame
#dataframe given by this function is converted into json and post to JS code .
def systemRating(df):
  final=pd.DataFrame()
  final["name"]=""
  final["google_rating"]=0
  final["system_rating"]=0
  for val in df["name"].unique():
    df2=df[df["name"]==val]
    count=sum=predictSum=averaged_rating=0
    for index, row in df2.iterrows():
      count=count+1
      sum=sum+row["google_rating"]
      averaged_rating=averaged_rating+row['averaged_rating']
      predictSum=predictSum+row["sentiment_score"]  
    averaged_rating=averaged_rating/count 
    ratings=(predictSum/count)+averaged_rating
    
    final=final.append({"name":val,"google_rating":df2["google_rating"].iloc[0],"system_rating":ratings,"averaged_rating":averaged_rating},ignore_index=True)
  return final

"""# **<h2>Scaling System Rating b/w 1-5**"""

def scale(ratings):
 
  scaler = MinMaxScaler(feature_range=(1, 5))
  orignal_system_rating=ratings+[0,7]
  system_ratings = np.array(orignal_system_rating).reshape(-1, 1)
  scaler = scaler.fit(np.array(system_ratings).reshape(-1, 1))
  scaled_ratings = scaler.transform(system_ratings)
  # system_ratings=[round(num, 1) for num in sclaed_ratings]

  # transferring back to list        
  system_ratings = scaled_ratings.tolist()
  system_ratings=[round(num, 1) for num in list(chain.from_iterable(system_ratings))]
 
  return system_ratings[:-2]

def addSystemRtaing(df1,df):
  for index,row in df.iterrows():
    df1['data'][index]['system_rating']=row['system_rating']
    df1['data'][index]['system_rating']=row['system_rating']
  return df1

"""## **Function to get 1KM points against Lat & Lng**"""

def add_blur(lat, long):
  meters = 1000
  earth_radius_in_km = 6378.137
  coeff = (1 / ((2 * math.pi / 360) * earth_radius_in_km)) / 1000
  blur_factor = meters * coeff

  p_lat = lat + blur_factor
  p_long = long + blur_factor / math.cos(lat * (math.pi / 180))

  n_lat = lat - blur_factor
  n_long = long - (blur_factor / math.cos(lat * (math.pi / 180)))

  return p_lat,p_long,n_lat,n_long

"""# **<h1>Creating a Flask API**"""

app=Flask(__name__)
run_with_ngrok(app)


def getLocations(lat,lng,category,actualLat,actualLng):
  global response,response1 
  api_key=''
  headers = {'Authorization': 'Bearer {}'.format(api_key)}
  search_api_url = ''%(category,lat,lng)
 
  if (lng and lat):
    response = requests.get(search_api_url, headers=headers)
    businesses=response.json()['businesses']
    for business in businesses:
      search_api_url = 'https://api.yelp.com/v3/businesses/%s/reviews'%(business['id'])
      response1 = requests.get(search_api_url, headers=headers)
      business['reviews']=response1.json()['reviews'] or []
      business['type']=category
      business['lat']=actualLat
      business['lng']=actualLng
    places={'data':businesses}
    
  return places


@app.route('/api/predictions',methods=[ 'POST'])
def basic():

  try:
    client = MongoClient("")
    db=client.get_database("Cleverus")
    records=db.businesses
  except:
    return jsonify({"error": "Connection Error",}), 502

  p_lat,p_lng,n_lat,n_lng=add_blur(request.json['latituide'],request.json['longitude'])
  data=records.find_one({"latitude":{"$gte":n_lat,"$lt":p_lat},"longitude":{"$gte":n_lng,"$lt":p_lng}})

  if (data==None):
    response=getLocations(request.json['latituide'],request.json['longitude'], request.json['category'],request.json['latituide'],request.json['longitude'])
    if not response['data']:
        return jsonify({"error": "No Data",}), 500
    df1=response
    df=jsonToDataframe(response)
    df['sentiment_score']=getPredictions(df)
    df=getNeutral(df)
    df=systemRating(df)
    df['system_rating']=scale(df['system_rating'].tolist())
    df1=addSystemRtaing(df1,df)
    return jsonify(df1)

  else:
    if (data[request.json['category']]!=[]):
     return jsonify({'data':data[request.json['category']]}),201
    else:
      response=getLocations(request.json['latituide'],request.json['longitude'], request.json['category'],data['latitude'],data['longitude'])
      if not response['data']:
        return jsonify({"error": "No Data",}), 500
      df1=response
      df=jsonToDataframe(response)
      df['sentiment_score']=getPredictions(df)
      df=getNeutral(df)
      df=systemRating(df)
      df['system_rating']=scale(df['system_rating'].tolist())
      df1=addSystemRtaing(df1,df)
      return jsonify(df1)


@app.route('/api/store',methods=['POST'])
def dataStore():
  try:
    client = MongoClient("Query String")
    db=client.get_database("Cleverus")
    records=db.businesses
  except:
    return jsonify({"error": "Connection Error",}), 502

  content=request.get_json(force=True)
  businessType=content["businessType"]
  content=content["data"]
  #print(businessType)
  if(content==None or businessType==None):
    return jsonify({"error": "Bad Request",}), 400
  
  lat=content["latitude"]
  lng=content["longitude"]
  p_lat,p_lng,n_lat,n_lng=add_blur(lat,lng)
  data=records.find_one({"latitude":{"$gte":n_lat,"$lt":p_lat},"longitude":{"$gte":n_lng,"$lt":p_lng}})
  if(data==None):
    for index,val in enumerate(content[businessType]):
      content[businessType][index]['lat']=content["latitude"]
      content[businessType][index]['lng']=content["longitude"]
    records.insert_one(content)
    return json.dumps({'success':True,'lat':content["latitude"],'lng':content["longitude"]}), 200, {'ContentType':'application/json'}
  else:
    for val in content[businessType]:
      val['lat']=data["latitude"]
      val['lng']=data["longitude"]
      data[businessType].append(val)
    records.update_one({'longitude':data["longitude"],'latitude':data["latitude"]},{"$set": { f"{businessType}":data[businessType] } })
    return json.dumps({'success':True,'lat':data["latitude"],'lng':data["longitude"]}), 200, {'ContentType':'application/json'}


@app.route('/api/fetch',methods=['GET'])
def datafetch():
  content=request.args
  lat=float(content['lat'])
  lng=float(content['lng'])
  business=content['business']
  id=content['id']
  if (lat==None or lng==None or business==None or id==None):
    return jsonify({"error": "Bad Request",}), 400
  try:
    client = MongoClient("")
    db=client.get_database("Cleverus")
    records=db.businesses
  except:
    return jsonify({"error": "Connection Error",}), 502
  finally:
    getData=records.find_one({"longitude":lng,"latitude":lat})
    if(getData==None):
      return jsonify({"error": "No Content",}), 204
    business_Data=getData[business]
    if(business_Data==None):
      return jsonify({"error": "No Content",}), 204
    for val in business_Data:
      if(val["id"]==id):
        return jsonify(val),200
    return jsonify({"error": "No Place Found",}), 204


@app.route('/api/randomBusiness',methods=['GET'])
def getRandomResturants():
  try:
    client = MongoClient("")
    db=client.get_database("Cleverus")
    records=db.businesses
  except:
    return jsonify({"error": "Connection Error",}), 502
  finally:
    data=list(records.find({"restaurants":{'$nin': [[], '']}}))  #Get documents where resturnats not null
    resturants=(random.choice(data))["restaurants"]          #Choose random document from all documents
    resturants.sort(key=lambda x: x['review_count'], reverse=True) #Sort on the basis of reviews count

    data=list(records.find({"hotels":{'$nin': [[], '']}}))  #Get documents where hotels not null
    hotels=(random.choice(data))["hotels"]          #Choose random document from all documents
    hotels.sort(key=lambda x: x['review_count'], reverse=True) #Sort on the basis of reviews count

    data_to_send={"resturants":resturants[1:4],"hotels":hotels[1:4]}
    return jsonify(data_to_send),200

  
@app.route('/api/allRandomBusiness',methods=['GET'])
def getAllRandomResturants():
  try:
    client = MongoClient("")
    db=client.get_database("Cleverus")
    records=db.businesses
  except:
    return jsonify({"error": "Connection Error",}), 502
  finally:
    data=list(records.find({"restaurants":{'$nin': [[], '']}}))  #Get documents where resturnats not null
    resturants=(random.choice(data))["restaurants"]          #Choose random document from all documents
    resturants.sort(key=lambda x: x['review_count'], reverse=True) #Sort on the basis of reviews count

    data=list(records.find({"hotels":{'$nin': [[], '']}}))  #Get documents where hotels not null
    hotels=(random.choice(data))["hotels"]          #Choose random document from all documents
    hotels.sort(key=lambda x: x['review_count'], reverse=True) #Sort on the basis of reviews count

    data=list(records.find({"hospitals":{'$nin': [[], '']}}))  #Get documents where resturnats not null
    hospitals=(random.choice(data))["hospitals"]          #Choose random document from all documents
    hospitals.sort(key=lambda x: x['review_count'], reverse=True) #Sort on the basis of reviews count

    data=list(records.find({"barbers":{'$nin': [[], '']}}))  #Get documents where hotels not null
    barbers=(random.choice(data))["barbers"]          #Choose random document from all documents
    barbers.sort(key=lambda x: x['review_count'], reverse=True)

    data_to_send={"resturants":resturants[1:4],"hotels":hotels[1:4],"hospitals":hospitals[1:4],"barbers":barbers[1:4]}
    return jsonify(data_to_send),200

    
@app.route('/api/similarBuisness',methods=['GET'])
def getRandomfromBusiness():
  content=request.args
  lat=float(content['lat'])
  lng=float(content['lng'])
  business=content['business']
  if (lat==None or lng==None or business==None):
    return jsonify({"error": "Bad Request",}), 400
  try:
    client = MongoClient("")
    db=client.get_database("Cleverus")
    records=db.businesses
  except:
    return jsonify({"error": "Connection Error",}), 502
  finally:
    getData=records.find_one({"longitude":lng,"latitude":lat})
    if(getData==None):
      return jsonify({"error": "No Content",}), 204
    business_Data=getData[business]
    if(business_Data==None):
      return jsonify({"error": "No Content",}), 204
    else:
      business_Data.sort(key=lambda x: x['review_count'], reverse=True)
      if(len(business_Data)>=3):
        return jsonify(business_Data[1:4]),200
      else:
        return jsonify(business_Data),200


@app.route('/api/businessCounts',methods=['GET'])
def getBusinessesCounts():
  try:
    client = MongoClient("")
    db=client.get_database("Cleverus")
    records=db.businesses
  except:
    return jsonify({"error": "Connection Error",}), 502
  finally:
    resturants=list(records.find({"restaurants":{'$nin': [None, '']}}))
    hotels=list(records.find({"hotels":{'$nin': [None, '']}}))
    barbers=list(records.find({"barbers":{'$nin': [None, '']}}))
    hospitals=list(records.find({"hospitals":{'$nin': [None, '']}}))
    resturants=[len(res['restaurants']) for res in resturants]
    hotels=[len(res['hotels']) for res in hotels]
    barbers=[len(res['barbers']) for res in barbers]
    hospitals=[len(res['hospitals']) for res in hospitals] 
    data_to_send={"resturants":sum(resturants),"hotels":sum(hotels),"barbers":sum(barbers),"hospitals":sum(hospitals)}
    return jsonify(data_to_send),200


@app.route('/api/reviewsCount',methods=['GET'])
def getReviewsCount():
  content=request.args
  id=content['userID']
  if (id==None):
    return jsonify({"error": "Bad Request",}), 400
  try:
    client = MongoClient("")
    db=client.get_database("Cleverus")
    records=db.users
  except:
    return jsonify({"error": "Connection Error",}), 502
  finally:
    getData=records.find_one({"userID":id})
    if(getData==None):
      return jsonify({"error": "No Content",}), 204
    else:
      return jsonify(getData['noOfReviews']),200


@app.route('/api/deleteUser',methods=['DELETE'])
def deleteUser():
  content=request.args
  id=content['userID']
  if (id==None):
    return jsonify({"error": "Bad Request",}), 400
  try:
    client = MongoClient("")
    db=client.get_database("Cleverus")
    records=db.users
  except:
    return jsonify({"error": "Connection Error",}), 502
  finally:
    getData=records.delete_one({"userID":id})
    return 'Deleted Successfully'


@app.route('/api/addReview',methods=['POST'])
def addReview():
  lat=float(request.json['latitude'])
  lng=float(request.json['longitude'])
  category=request.json['category']
  id=request.json['id']
  userID=request.json['userID']
  review=request.json['review']
  lang=None
  try:
    lang =detect(review['text'])
  except:
    return jsonify({"error": "Not a language",}), 205
  
  if(lang!='en'):
    return jsonify({"error": "Language must be English",}), 205
  else:
    if (lat==None or lng==None or category==None or id==None or review==None or userID==None):
      return jsonify({"error": "Bad Request",}), 400
    try:
      client = MongoClient("")
      db=client.get_database("Cleverus")
      records=db.businesses
    except:
      return jsonify({"error": "Connection Error",}), 502
    finally:
      getData=records.find_one({"longitude":lng,"latitude":lat})
      if(getData==None):
        return jsonify({"error": "No Content",}), 204
      business_Data=getData[category]
      if(business_Data==None):
        return jsonify({"error": "No Content for this Category",}), 204
      else:
        for i,val in enumerate(business_Data):
          if (val['id']==id):
            reviews=val['reviews']
            reviews.append(review)
            business_Data[i]['reviews']= reviews
            db.businesses.update_one({'longitude':lng,'latitude':lat},{"$set": { f"{category}":business_Data } })
            user=db.users.find_one({"userID":userID})
            if (user==None):
              user={"userID":userID, "favPlaces":[],"noOfReviews":1}
              db.users.insert_one(user)  
            else:
              db.users.update_one({'userID':userID},{"$inc":{'noOfReviews':1}})
            return 'Done',200
      return jsonify({"error": "No Place Found",}), 204


@app.route('/api/addFavPlace',methods=['POST'])
def addFavPlace():
  content=request.get_json(force=True)
  userID=content["userID"]
  favPlace=content["favPlaces"]
  if(userID==None or favPlace==None):
    return jsonify({"error": "No Content",}), 204
  try:
    client = MongoClient("")
    db=client.get_database("Cleverus")
    records=db.users
  except:
    return jsonify({"error": "Connection Error",}), 502

  user=records.find_one({"userID":userID})
  if(user==None):
    user={
        "userID":userID,
        "favPlaces":favPlace,
        "noOfReviews":0
    }
    records.insert_one(user)
    return jsonify({}), 200
  else:
    for val in favPlace:
      if [fav for fav in user["favPlaces"] if fav['id'] == val['id']]:
        return jsonify({}), 201
    user["favPlaces"].append(val)
    records.update_one({"userID":userID},{"$set":{"favPlaces":user["favPlaces"]}})
    return jsonify({}), 200

@app.route('/api/favPlaceExist',methods=['POST'])
def checkFavPlace():
  content=request.get_json(force=True)
  userID=content["userID"]
  favPlace=content["favPlaces"]
  if(userID==None or favPlace==None):
    return jsonify({"error": "No Content",}), 204
  try:
    client = MongoClient("")
    db=client.get_database("Cleverus")
    records=db.users
  except:
    return jsonify({"error": "Connection Error",}), 502

  user=records.find_one({"userID":userID})
  if(user==None):
    return jsonify({}), 200
  else:
    for val in favPlace:
      if [fav for fav in user["favPlaces"] if fav['id'] == val['id']]:
        return jsonify({}), 201
    return jsonify({}), 200


@app.route('/api/getFavPlace',methods=['GET'])
def getFavPlace():
  content=request.args
  userID=content['userID']
  if(userID==None):
    return jsonify({"error": "No Content",}), 204
  try:
    client = MongoClient("")
    db=client.get_database("Cleverus")
    records=db.users
  except:
    return jsonify({"error": "Connection Error",}), 502

  user=records.find_one({"userID":userID})
  if(user==None):
    return jsonify([]),200
  else:
    return jsonify(user["favPlaces"]),200

@app.route('/api/updateRating',methods=['POST'])
def makeRatingDynamic():
  global updated_ratings_df,updated_df1;
  content=request.get_json(force=True)
  lat=float(content['lat'])
  lng=float(content['lng'])
  business=content['business']
  id=content['id']
  if (lat==None or lng==None or business==None or id==None):
    return jsonify({"error": "Bad Request",}), 400
  try:
    client = MongoClient("")
    db=client.get_database("Cleverus")
    records=db.businesses
  except:
    return jsonify({"error": "Connection Error",}), 502
  finally:
    getData=records.find_one({"longitude":lng,"latitude":lat})
    if(getData==None):
      return jsonify({"error": "No Content",}), 204
    business_Data=getData[business]
    if(business_Data==None):
      return jsonify({"error": "No Content",}), 204
    for i,val in enumerate(business_Data):
      if(val["id"]==id):
        df1={'data': [val]}
        df=jsonToDataframe(df1)
        df['sentiment_score']=getPredictions(df)
        df=getNeutral(df)      
        df=systemRating(df)
        df['system_rating']=scale(df['system_rating'].tolist())
        df1=addSystemRtaing(df1,df)
        business_Data[i]['system_rating']=df1['data'][0]['system_rating']
        db.businesses.update_one({'longitude':lng,'latitude':lat},{"$set": { f"{business}":business_Data } })
        return jsonify(business_Data[i]),200     

app.run()